\documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{graphicx}
\usepackage[left=1in,right=1in,top=1in,bottom=1in]{geometry}

\author{Luke Fraser}
\title{Multi-Person and Multi-Robot Behaviour Recognition Room using N-Time of Flight Cameras}
\begin{document}
\maketitle
\begin{abstract}
Currently systems have been developed to track people and provide low level feedback in relatively small indoor environments with occlusions using multiple RGB-D sensors. This project aims to develop a multi-RGB-D sensor system that tracks multiple people, identifies pose, identifies people, in larger indoor environments with occlusions and feeds back high-level behavior recognition to robots for Human-Robot interactions. The project will help make multi-person and multi-robot interactions in cooperative environments easier and more feasible. The two components of the system, people tracking and behavior recognition will be validated against hand tracked and recognized footage.
\end{abstract}

\section{Motivation}
Human-Robot interaction is a growing field of robotics and multi-robot and multi-person cooperative environments are an important area of robotics research \cite{5928680, Schultz:2005:TCR:1052438.1052456}. Efforts to develop robots that are cooperative with people in different environments is the focus of much research. Robot cooperation has the potential to provide a symbiotic relationship with people, where the robot is able to perform more complicated tasks with human aid and the human benefits from assistance of the robot. Complex tasks are more obtainable for robots with cooperation\cite{6631192}. Just as people cooperate to complete complex tasks, so can robots.

Human-Robot interactions require complex knowledge about the environment and the people involved in the interactions in order to make sophisticated decisions. A robot will typically have on board sensors to understand its environment. Laser scanners, Microsoft kinect, and stereo cameras are some of the sensors commonly used to understand a 3D environment and track the position of human participants. The requirements to understand the position of people involved in an interaction is a complex task. Tracking people requires computation and in many cases is not the underlying purpose of a given interaction. Position information of people in a given scene is necessary for human interactions to take place.

With real-time knowledge of a persons position and the position of their limbs behavior recognition can be performed\cite{journals/ijsr/MeadAM13}. Behavior recognition is an important component of 
\section{Goals}
\section{Related Work}
Today one common issue with RGB-D sensors is the limitation of positioning in an environment. Due to the fact that most RGB-D Sensors like the kinect for instance, use an IR pattern at a certain frequency to understand the scene depth. This poses a problem when two kinects project on the same objects where one kinect can see the IR pattern of the other making the signal ambiguous. Some attempts to vibrate each additional kinect at different frequencies have been used to allow the different IR fields to be in a different phase out of view of each other.
\section{Proposed Research}
I would like to attempt to use the new Kinect one sensor instead of the IR techonology in the hopes to produce a system capable of using N-TOFsensors in an arbitrary space to perform behavior analysis for HRI.

This will provide a cheaper way for researchers to analyze Human robot interactions and have feedback to robots about a persons current state in a given interaction. This will allow for an easy setup of sensors that give a robot far more information about a given HRI without the need to perform the computation on the robot itself. This means the current state of the human behavior as well as accurate human position will be known by robots in larger spaces than possible with a single kinect. This will avoid the necessity for complex tracking markers on participants. There has been a lot of work on tracking multiple people in RGB-D environments. This will also allow for multi-robot and multi-person interactions to take place in a more efficient manor that avoid each robot performing redundant computation to understand the location of each person in a given scenario. The end goal is to provide a fused scene where each person has a skeleton representing their position.

I will use known a priori objects and place them in the scene. I will then compare the accuracy of the position as well as shape of the registered objects. This can also be done with a person but with less accuracy. I can also compare my method against other published methods.
\section{Expected Outcomes}
\section{Time-line}

\bibliography{references}
\bibliographystyle{plain}
\newpage
\section{Publication Plan}
The results of this work will be submitted to robotics conferences such as:
\begin{itemize}
 \item RSS - Robotics Science and Systems
 \item IROS - Intelligent Robots and Systems
 \item ICRA - International Conference on Robotics and Automation
 \item RO-MAN - International Symposium on Robot and Human Interactive Communication
\end{itemize}

\section{Transcript}
\section{Endorsement Letters}
\section{CV}
\section{Statement of Career Goals}

\end{document}